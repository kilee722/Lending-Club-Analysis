{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.\tOpening File and Joining Other Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tLibrary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tOpening Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2260668, 145)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"loan.csv\", low_memory = False)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.\tChanging Several Columnsâ€™ Data Type and Adding Needed Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change certain columns to date time\n",
    "\n",
    "## Getting the year out of the month-year format in the column\n",
    "df['earliest_cr_line_year'] = df['earliest_cr_line'].str.strip().str[-4:].fillna(0).astype('int')\n",
    "\n",
    "## Changing earliest_cr_line into pd_datetime format\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "\n",
    "## Changing issue_d into pd_datetime format\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
    "\n",
    "# Create new column in loan data of previous quarter\n",
    "df['pqissue_d'] = df['issue_d'] - pd.tseries.offsets.DateOffset(months = 3)\n",
    "\n",
    "# Create new column in loan data of current quarter of issue date\n",
    "df['issue_q'] = pd.PeriodIndex(pd.to_datetime(df['issue_d']), freq = 'Q')\n",
    "\n",
    "# Create new column in loan data for quarter of issue_date minus 1 quarter - previous quarter issue quarter\n",
    "df['pqissue_q'] = pd.PeriodIndex(pd.to_datetime(df['pqissue_d']), freq = 'Q')\n",
    "\n",
    "# Change earliest_credit_line and issue_date to correct datetime format to calculate age of Credit\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
    "\n",
    "# Change last payment date to datetime format\n",
    "df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate age of credit in year\n",
    "df['ageOfCredit'] = \n",
    "((df['issue_d']-df['earliest_cr_line']))/np.timedelta64(1,'Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming employment length longer than 10 years to 10 years and less than 1 years to 0 years\n",
    "df['emp_length'].replace(to_replace='10+ years', value='10 years', inplace=True)\n",
    "df['emp_length'].replace('< 1 year', '0 years', inplace=True)\n",
    "\n",
    "\n",
    "def emp_length_to_int(s):\n",
    "    if pd.isnull(s):\n",
    "        return s\n",
    "    else:\n",
    "        return np.int8(s.split()[0])\n",
    "\n",
    "df['emp_length'] = df['emp_length'].apply(emp_length_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log base 10 the two numeric variables for easier visualizations\n",
    "df['log_annual_inc'] = df['annual_inc'].apply(lambda x: np.log10(x+1))\n",
    "df['log_revol_bal'] = df['revol_bal'].apply(lambda x: np.log10(x+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.\tOpening Other Data Set for Easier Join of State Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open csv state abbrv to state name\n",
    "sn = pd.read_csv(\"https://worldpopulationreview.com/static/states/abbr-name.csv\", header = None)\n",
    "\n",
    "# Rename column \n",
    "sn = sn.rename(columns = {0: \"addr_state\" , 1:\"length_as\"})\n",
    "\n",
    "# Merge\n",
    "df = df.merge(sn, how = 'left', on = 'addr_state')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the o in the new joined data set to capital O for joining with other data sets\n",
    "df.loc[df['length_as'] == 'District Of Columbia', 'length_as'] = 'District of Columbia'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.\tOpening Other Data Set for Joining GDP Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdp open\n",
    "gdp = pd.read_csv(\"SQGDP1__ALL_AREAS_2005_2020.csv\")\n",
    "\n",
    "# get only real GDP description\n",
    "gdp = gdp.loc[gdp['Description'] == 'Real GDP (millions of chained 2012 dollars)']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "gdp = gdp.drop(columns = ['GeoFIPS', 'Region', 'TableName', 'LineCode', 'IndustryClassification', 'Description', 'Unit'])\n",
    "\n",
    "# Readjust into time-series table\n",
    "gdp = gdp.melt(id_vars = ['GeoName'],\n",
    "         var_name = \"YEAR:Q\",\n",
    "         value_name = \"RealGDP\")\n",
    "\n",
    "# Change time period into datetime pandas format\n",
    "gdp['YEAR:Q'] = gdp['YEAR:Q'].str.replace(r'(\\d+):(Q\\d)', r'\\1-\\2')\n",
    "gdp['startQuarter'] = pd.to_datetime(gdp['YEAR:Q'])\n",
    "gdp['endQuarter'] = pd.to_datetime(gdp['startQuarter'] + pd.tseries.offsets.QuarterEnd(0))\n",
    "\n",
    "\n",
    "# Sort values\n",
    "gdp = gdp.sort_values(by = ['GeoName', 'YEAR:Q'])\n",
    "\n",
    "# Get first difference percentage change\n",
    "gdp['Diff'] = gdp.groupby(['GeoName'])['RealGDP'].pct_change().fillna(0)\n",
    "\n",
    "# Get previous quarter in 2 new columns\n",
    "gdp['prevQuarterStartDate'] = gdp['startQuarter'] - pd.tseries.offsets.DateOffset(months = 3)\n",
    "gdp['prevQuarterEndDate'] = gdp['endQuarter'] - pd.tseries.offsets.DateOffset(months = 3)\n",
    "gdp['prevQuarterDiff'] = gdp.groupby(['GeoName'])['Diff'].shift(1)\n",
    "gdp['issue_q'] = pd.PeriodIndex(pd.to_datetime(gdp['startQuarter']), freq = 'Q')\n",
    "gdp['pqissue_q'] = pd.PeriodIndex(pd.to_datetime(gdp['prevQuarterStartDate']), freq = 'Q')\n",
    "\n",
    "# Check for final and ready to join\n",
    "gdp = gdp.rename(columns = {'GeoName':\"length_as\"})\n",
    "gdp = gdp[['length_as', 'issue_q', 'Diff', 'pqissue_q', 'prevQuarterDiff']]\n",
    "gdp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.\tJoining with GDP Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, gdp, on = ['length_as', 'issue_q'], how = 'left')\n",
    "df = df.drop(columns = 'pqissue_q_x')\n",
    "df = df.rename(columns = {'pqissue_q_y':'pqissue_q'})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  G.\tOpening Other Data Set for Joining Unemployment Rate Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bls = pd.read_excel('ststdsadata.xlsx')\n",
    "\n",
    "# Renaming the state to it's abbreviation \n",
    "bls.loc[bls['Unnamed: 1'] == \"Alabama\", ['Unnamed: 1']] =  'AL'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Alaska\", ['Unnamed: 1']] =  'AK'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Arizona\", ['Unnamed: 1']] =  'AZ'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Arkansas\", ['Unnamed: 1']] =  'AR'\n",
    "bls.loc[bls['Unnamed: 1'] == \"California\", ['Unnamed: 1']] =  'CA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Colorado\", ['Unnamed: 1']] =  'CO'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Connecticut\", ['Unnamed: 1']] =  'CT'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Delaware\", ['Unnamed: 1']] =  'DE'\n",
    "bls.loc[bls['Unnamed: 1'] == \"District of Columbia\", ['Unnamed: 1']] =  'DC'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Florida\", ['Unnamed: 1']] =  'FL'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Georgia\", ['Unnamed: 1']] =  'GA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Hawaii\", ['Unnamed: 1']] =  'HI'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Idaho\", ['Unnamed: 1']] =  'ID'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Illinois\", ['Unnamed: 1']] =  'IL'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Indiana\", ['Unnamed: 1']] =  'IN'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Iowa\", ['Unnamed: 1']] =  'IA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Kansas\", ['Unnamed: 1']] =  'KS'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Kentucky\", ['Unnamed: 1']] =  'KY'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Louisiana\", ['Unnamed: 1']] =  'LA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Maine\", ['Unnamed: 1']] =  'ME'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Maryland\", ['Unnamed: 1']] =  'MD'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Massachusetts\", ['Unnamed: 1']] =  'MA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Michigan\", ['Unnamed: 1']] =  'MI'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Minnesota\", ['Unnamed: 1']] =  'MN'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Mississippi\", ['Unnamed: 1']] =  'MS'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Missouri\", ['Unnamed: 1']] =  'MO'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Montana\", ['Unnamed: 1']] =  'MT'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Nebraska\", ['Unnamed: 1']] =  'NE'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Nevada\", ['Unnamed: 1']] =  'NV'\n",
    "bls.loc[bls['Unnamed: 1'] == \"New Hampshire\", ['Unnamed: 1']] =  'NH'\n",
    "bls.loc[bls['Unnamed: 1'] == \"New Jersey\", ['Unnamed: 1']] =  'NJ'\n",
    "bls.loc[bls['Unnamed: 1'] == \"New Mexico\", ['Unnamed: 1']] =  'NM'\n",
    "bls.loc[bls['Unnamed: 1'] == \"New York\", ['Unnamed: 1']] =  'NY'\n",
    "bls.loc[bls['Unnamed: 1'] == \"North Carolina\", ['Unnamed: 1']] =  'NC'\n",
    "bls.loc[bls['Unnamed: 1'] == \"North Dakota\", ['Unnamed: 1']] =  'ND'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Ohio\", ['Unnamed: 1']] =  'OH'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Oklahoma\", ['Unnamed: 1']] =  'OK'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Oregon\", ['Unnamed: 1']] =  'OR'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Pennsylvania\", ['Unnamed: 1']] =  'PA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Rhode Island\", ['Unnamed: 1']] =  'RI'\n",
    "bls.loc[bls['Unnamed: 1'] == \"South Carolina\", ['Unnamed: 1']] =  'SC'\n",
    "bls.loc[bls['Unnamed: 1'] == \"South Dakota\", ['Unnamed: 1']] =  'SD'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Tennessee\", ['Unnamed: 1']] =  'TN'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Texas\", ['Unnamed: 1']] =  'TX'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Utah\", ['Unnamed: 1']] =  'UT'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Vermont\", ['Unnamed: 1']] =  'VT'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Virginia\", ['Unnamed: 1']] =  'VA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Washington\", ['Unnamed: 1']] =  'WA'\n",
    "bls.loc[bls['Unnamed: 1'] == \"West Virginia\", ['Unnamed: 1']] =  'WV'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Wisconsin\", ['Unnamed: 1']] =  'WI'\n",
    "bls.loc[bls['Unnamed: 1'] == \"Wyoming\", ['Unnamed: 1']] =  'WY'\n",
    "\n",
    "# Creating Month column \n",
    "df['Month'] = pd.DatetimeIndex(df['issue_d']).month\n",
    "\n",
    "# Converting to leading 0, because that's the format used in BLS dataset \n",
    "# for instance, January is 01 instead of 1\n",
    "df[\"Month\"] = df.Month.map(\"{:02}\".format)\n",
    "\n",
    "# Creating Year column in df \n",
    "df['Year'] = pd.DatetimeIndex(df['issue_d']).year\n",
    "\n",
    "# renaming columns in bls dataset \n",
    "bls = bls.rename(columns = {\"Unnamed: 1\": \"addr_state\"})\n",
    "bls = bls.rename(columns = {\"Unnamed: 2\": \"Year\"})\n",
    "bls = bls.rename(columns = {\"Unnamed: 3\": \"Month\"})\n",
    "bls = bls.rename(columns = {\"Unnamed: 10\": \"UR\"}) #unemployment rate\n",
    "\n",
    "bls = pd.DataFrame(bls, columns=['addr_state', 'Year', 'Month', 'UR'])\n",
    "bls = bls.dropna() #CLEANED bls \n",
    "bls.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns for better joining\n",
    "bls['YearMonth'] = bls['Year'].astype(str)+'-'+bls['Month'].astype(str)\n",
    "bls['YearQuarter'] = pd.PeriodIndex(pd.to_datetime(bls['YearMonth']), freq = 'Q')\n",
    "\n",
    "# Sorting values to calculate difference between one period to another\n",
    "bls = bls.sort_values(by = ['addr_state', 'YearMonth'])\n",
    "bls['URDiff'] = bls.groupby(['addr_state'])['UR'].pct_change().fillna(0)\n",
    "\n",
    "# Sorting values to calculate difference between previous period and two periods ago\n",
    "bls['URprevMonthDiff'] = bls.groupby(['addr_state'])['URDiff'].shift(1)\n",
    "bls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H.\tJoining with Unemployment Rate Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bls['Year']=bls['Year'].astype(int) #has to convert to allow merging; datatypes have to be the same. \n",
    "df = pd.merge(df, bls, on = ['Month', 'Year', 'addr_state'], how = 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.\tTransfer to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II.\tData Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tLibrary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tOpening Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the data frame after joining (resultant of previous section)\n",
    "df = pd.read_csv(\"df.csv\", low_memory = False)\n",
    "\n",
    "# Removing that first column: (\"Unnamed: 0\")\n",
    "df = df.drop(columns = \"Unnamed: 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.\tDealing with Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to train our model to evaluate charged-off (failing loans) or fully paid (successful loans)\n",
    "# Thus, we want to separate all other loan status categories from these two categories\n",
    "\n",
    "print(df['loan_status'].describe(), \"\\n\")\n",
    "print(\"Before removing other categories\")\n",
    "print(df['loan_status'].value_counts(dropna = False))\n",
    "df = df.loc[df['loan_status'].isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "print(\"After removing other categories\", \"\\n\")\n",
    "print(df['loan_status'].value_counts(dropna = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.\tFiltering Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is available to investor\n",
    "keep_list1 = ['loan_amnt', 'issue_d', 'loan_status', 'funded_amnt', \n",
    "              'funded_amnt_inv', 'verification_status', 'installment', 'grade', \n",
    "              'sub_grade', 'home_ownership', 'emp_length', 'emp_title', \n",
    "              'addr_state', 'zip_code', 'annual_inc', 'dti', \n",
    "              'ageOfCredit', 'earliest_cr_line', 'earliest_cr_line_year',\n",
    "              'open_acc', 'total_acc', 'revol_bal', 'revol_util', \n",
    "              'inq_last_6mths', 'acc_now_delinq', 'delinq_amnt', 'delinq_2yrs', \n",
    "              'pub_rec', 'collections_12_mths_ex_med', 'int_rate', \n",
    "              'tot_coll_amt', 'purpose', 'term', 'initial_list_status',    \n",
    "              'application_type', 'length_as', 'pqissue_d', 'issue_q',\n",
    "              'log_annual_inc', 'log_revol_bal', 'Diff', 'pqissue_q', \n",
    "              'prevQuarterDiff', 'Month', 'Year', 'UR', 'YearMonth',\n",
    "              'YearQuarter', 'URDiff', 'URprevMonthDiff']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting after correlation analysis and Cramer's V\n",
    "keep_list3 = ['Month', 'loan_amnt', 'issue_d', 'loan_status',\n",
    "              'verification_status', 'grade', 'home_ownership', 'emp_length', \n",
    "              'addr_state', 'annual_inc', 'log_annual_inc', 'dti',\n",
    "              'ageOfCredit', 'open_acc', 'total_acc', 'revol_bal', \n",
    "              'revol_util', 'acc_now_delinq', 'delinq_amnt', 'delinq_2yrs',  \n",
    "              'pub_rec', 'collections_12_mths_ex_med',\n",
    "              'int_rate', 'inq_last_6mths', 'tot_coll_amt', 'purpose', 'term',\n",
    "              'initial_list_status', 'application_type',\n",
    "              'log_revol_bal', 'Diff', 'prevQuarterDiff', 'UR', 'URDiff',\n",
    "              'URprevMonthDiff']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 will contain whatever in keep_list1 - before correlation analysis\n",
    "# df2 will contain whatever in keep_list3 - after correlation analysis\n",
    "# We discard keep_list2 BECAUSE it's not available to potential investor in the GUI\n",
    "df1 = df[keep_list1]\n",
    "df2 = df[keep_list3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NA\n",
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()\n",
    "\n",
    "print(\"DF1 shape is: \", df1.shape)\n",
    "print(\"DF2 shape is: \", df2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.\tCreating Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_with_dummies = df2.copy()\n",
    "# Creating a category for charged off or not\n",
    "df2_with_dummies['charged_off'] = (df2_with_dummies['loan_status'] == 'Charged Off').apply(np.uint8)\n",
    "\n",
    "# Change grade into different kinds of dummies\n",
    "df2_with_dummies['grade'] = pd.factorize(df2_with_dummies['grade'])[0] + 1\n",
    "\n",
    "# Get dummies for certain variables only\n",
    "df2_with_dummies = pd.get_dummies(df2_with_dummies,columns=['verification_status', 'home_ownership', 'addr_state', 'purpose', 'term', 'initial_list_status',\n",
    "                           'application_type'], \n",
    "               drop_first = True,\n",
    "               dummy_na = True)\n",
    "\n",
    "# Getting observations between 2015 and 2018\n",
    "df2_with_dummies = df2_with_dummies[(df2_with_dummies['issue_d'] >= '2015-01-01') & (df2_with_dummies['issue_d'] <= '2018-12-31')]\n",
    "# Dropping issue date because it's unnecessary\n",
    "df2_with_dummies.drop(columns = ['issue_d', 'loan_status'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "df2_with_dummies.shape\n",
    "\n",
    "Results:\n",
    "(797003, 108)\n",
    "\n",
    "# Create new CSV\n",
    "df2_with_dummies.to_csv('df2_with_dummies.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.\tPrincipal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tLibrary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics, datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tOpening Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "df2_with_dummies = pd.read_csv(\"df2_with_dummies.csv\", low_memory = False)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# annual_inc and revol_bal are replaced with its log version\n",
    "# month isn't used\n",
    "df2_with_dummies = df2_with_dummies.drop(columns = [\"Unnamed: 0\", 'annual_inc', 'revol_bal', 'Month'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.\tSelecting Numerical Variables to Scale and Run PCA on the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "Scaled = ['loan_amnt', 'emp_length', 'log_annual_inc', 'dti', 'ageOfCredit',\n",
    "          'open_acc', 'total_acc', 'revol_util', 'inq_last_6mths',          \n",
    "          'acc_now_delinq', 'delinq_amnt', 'delinq_2yrs', 'pub_rec',\n",
    "          'collections_12_mths_ex_med', 'int_rate', 'tot_coll_amt',\n",
    "          'log_revol_bal', 'Diff', 'prevQuarterDiff', 'UR', 'URDiff',  \n",
    "           'URprevMonthDiff']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_with_dummies[Scaled] = StandardScaler().fit_transform(df2_with_dummies[Scaled])\n",
    "df2_with_dummies\n",
    "\n",
    "# Create a copy for PCA\n",
    "df2_with_dummies_PCA = df2_with_dummies.copy()\n",
    "\n",
    "# Isolate target variable\n",
    "y = df2_with_dummies_PCA['charged_off']\n",
    "x = df2_with_dummies_PCA.drop(columns = ['charged_off'])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "x = pca.fit_transform(x)\n",
    "\n",
    "# Check the x aftermath - a bunch of eigenvector\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to determine elbow\n",
    "ev = pca.explained_variance_ratio_\n",
    "print(ev)\n",
    "plt.plot(ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to cumulative sum\n",
    "plt.plot(ev.cumsum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate target variable\n",
    "y = df2_with_dummies_PCA['charged_off']\n",
    "x = df2_with_dummies_PCA.drop(columns = ['charged_off'])\n",
    "\n",
    "# Apply PCA 25\n",
    "pca = PCA(n_components=25)\n",
    "x = pca.fit_transform(x)\n",
    "\n",
    "# Print to determine elbow\n",
    "ev = pca.explained_variance_ratio_\n",
    "print(ev)\n",
    "plt.plot(ev)\n",
    "\n",
    "# Getting cumulative sum\n",
    "plt.plot(ev.cumsum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export to csv\n",
    "pd.DataFrame(x).to_csv(\"x_with_dummies_PCA25.csv\")\n",
    "pd.DataFrame(y).to_csv(\"y_with_dummies_PCA25.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV.\tMachine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tLibrary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics, datasets,tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tOpening Files and Separating Target and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df2_with_dummies.csv\")\n",
    "df = df.drop(columns = ['Unnamed: 0','log_annual_inc', 'revol_bal', 'UR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining target and feature variable\n",
    "X = df.drop(columns = 'charged_off')\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y = df['charged_off']\n",
    "\n",
    "# splitting dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =.3,random_state=1234, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.\tNaive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bayesian Classifier instance for classification\n",
    "gnb = GaussianNB() \n",
    "\n",
    "# Build a Bayesian Classification Model and predict the type using the test data.\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_nb = gnb.predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred_nb)\n",
    "print(metrics.classification_report(y_test,y_pred_nb))\n",
    "\n",
    "print('PERFORMANCE SCORE: NB ENTIRE DATASET')\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_nb))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_nb))\n",
    "print ('Recall:', recall_score(y_test, y_pred_nb))\n",
    "print ('Precision:', precision_score(y_test, y_pred_nb))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_nb))\n",
    "\n",
    "mse_nb = mean_squared_error(y_test, y_pred_nb)\n",
    "rmse_nb = math.sqrt(mse_nb)\n",
    "print(rmse_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.\tDecision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Create a model (object) for classification\n",
    "dtm = DecisionTreeClassifier()\n",
    " \n",
    "# Build a decision tree\n",
    "dtm.fit(X_train, y_train)\n",
    "y_pred_dt = dtm.predict(X_test)\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = dtm.score(X_test, y_test)\n",
    "\n",
    "print (metrics.classification_report(y_test,y_pred_dt))\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_dt))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_dt))\n",
    "print ('Recall:', recall_score(y_test, y_pred_dt))\n",
    "print ('Precision:', precision_score(y_test, y_pred_dt))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_dt))\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "rmse_dt = math.sqrt(mse_dt)\n",
    "print(rmse_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.\tRandom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model (object) for classification\n",
    "rfcm = RandomForestClassifier()\n",
    "\n",
    "# Build a random forest classification model\n",
    "rfcm.fit(X_train, y_train)\n",
    "y_pred_rf = rfcm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "#accuracy = rfcm.score(X_test, y_test)\n",
    "#print('Accuracy: {0:.2f}'.format(accuracy))\n",
    "\n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm1 = metrics.confusion_matrix(y_test,y_pred_rf)\n",
    "print('\\nConfusion Matrix','\\n',cm1)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test,y_pred_rf))\n",
    "\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_rf))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_rf))\n",
    "print ('Recall:', recall_score(y_test, y_pred_rf))\n",
    "print ('Precision:', precision_score(y_test, y_pred_rf))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_rf))\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = math.sqrt(mse_rf)\n",
    "print(rmse_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.\tNeural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "Xn = scale(X)\n",
    " \n",
    "# Set the 'stratify' option 'y' to sample \n",
    "Xn_train, Xn_test = train_test_split(Xn, test_size =.3,random_state=1234, stratify=y)\n",
    "y_train, y_test = train_test_split(y, test_size=.3, random_state=1234, stratify=y)\n",
    "nnm = MLPClassifier(hidden_layer_sizes=(20,), max_iter=1000,activation='logistic')\n",
    "\n",
    "# Make predictions\n",
    "nnm.fit(Xn_train, y_train)\n",
    "y_pred_NN = nnm.predict(Xn_test)\n",
    "\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test,y_pred_NN))\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_NN))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_NN))\n",
    "print ('Recall:', recall_score(y_test, y_pred_NN))\n",
    "print ('Precision:', precision_score(y_test, y_pred_NN))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_NN))\n",
    "mse_nn = mean_squared_error(y_test, y_pred_NN)\n",
    "rmse_nn = math.sqrt(mse_nn)\n",
    "print(rmse_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G.\tNaive Bayesian Using PCA25 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining target and feature variable\n",
    "X = pd.read_csv(\"x_with_dummies_PCA25.csv\")\n",
    "X = X.drop(columns = 'Unnamed: 0')\n",
    "y = pd.read_csv(\"y_with_dummies_PCA25.csv\")\n",
    "y = y.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# splitting dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =.3,random_state=1234, stratify=y)\n",
    "\n",
    "# Create a Bayesian Classifier instance for classification\n",
    "gnb_PCA = GaussianNB() \n",
    "\n",
    "# Build a Bayesian Classification Model and predict the type using the test data.\n",
    "gnb_PCA.fit(X_train, y_train)\n",
    "y_pred_nb_pca = gnb_PCA.predict(X_test)\n",
    "\n",
    "cm_nb_pca = metrics.confusion_matrix(y_test,y_pred_nb_pca)\n",
    "print(metrics.classification_report(y_test,y_pred_nb_pca))\n",
    "\n",
    "print('PERFORMANCE SCORE: NB ENTIRE DATASET')\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_nb_pca))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_nb_pca))\n",
    "print ('Recall:', recall_score(y_test, y_pred_nb_pca))\n",
    "print ('Precision:', precision_score(y_test, y_pred_nb_pca))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_nb_pca))\n",
    "mse_nb_pca = mean_squared_error(y_test, y_pred_nb_pca)\n",
    "rmse_nb_pca = math.sqrt(mse_nb_pca)\n",
    "print('RMSE:', rmse_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H.\tDecision Tree Using PCA25 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_pca = DecisionTreeClassifier()\n",
    " \n",
    "# Build a decision tree\n",
    "dtm_pca.fit(X_train, y_train)\n",
    "y_pred_dt_pca = dtm_pca.predict(X_test)\n",
    "\n",
    "cm_dt_pca = metrics.confusion_matrix(y_test,y_pred_dt_pca)\n",
    "print(metrics.classification_report(y_test,y_pred_dt_pca))\n",
    "\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_dt_pca))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_dt_pca))\n",
    "print ('Recall:', recall_score(y_test, y_pred_dt_pca))\n",
    "print ('Precision:', precision_score(y_test, y_pred_dt_pca))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_dt_pca))\n",
    "mse_dt_pca = mean_squared_error(y_test, y_pred_dt_pca)\n",
    "rmse_dt_pca = math.sqrt(mse_dt_pca)\n",
    "print(rmse_dt_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.\tRandom Forest Using PCA25 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a model (object) for classification\n",
    "rfcm_pca = RandomForestClassifier()\n",
    "\n",
    "# Build a random forest classification model\n",
    "rfcm_pca.fit(X_train, y_train)\n",
    "y_pred_rf_pca = rfcm_pca.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test,y_pred_rf_pca))\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_rf_pca))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_rf_pca))\n",
    "print ('Recall:', recall_score(y_test, y_pred_rf_pca))\n",
    "print ('Precision:', precision_score(y_test, y_pred_rf_pca))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_rf_pca))\n",
    "mse_rf_pca = mean_squared_error(y_test, y_pred_rf_pca)\n",
    "rmse_rf_pca = math.sqrt(mse_rf_pca)\n",
    "print(rmse_rf_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J.\tNeural Network Using PCA25 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "Xn = scale(X)\n",
    " \n",
    "# Set the 'stratify' option 'y' to sample \n",
    "Xn_train, Xn_test = train_test_split(Xn, test_size =.3,random_state=1234, stratify=y)\n",
    "y_train, y_test = train_test_split(y, test_size=.3, random_state=1234, stratify=y)\n",
    "\n",
    "nnm_PCA = MLPClassifier(hidden_layer_sizes=(20,), max_iter=1000,activation='logistic')\n",
    "\n",
    "# Make predictions\n",
    "nnm_PCA.fit(Xn_train, y_train)\n",
    "y_pred_NN_PCA = nnm_PCA.predict(Xn_test)\n",
    "\n",
    "print('PERFORMANCE SCORE NN- ENTIRE DATASET')\n",
    "print(metrics.classification_report(y_test,y_pred_rf_pca), \"\\n\")\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_NN_PCA))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_NN_PCA))\n",
    "print ('Recall:', recall_score(y_test, y_pred_NN_PCA))\n",
    "print ('Precision:', precision_score(y_test, y_pred_NN_PCA))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_NN_PCA))\n",
    "mse_nn_pca = mean_squared_error(y_test, y_pred_NN_PCA)\n",
    "rmse_nn_pca = math.sqrt(mse_nn_pca)\n",
    "print(rmse_nn_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V.\tMachine Learning Algorithms Using Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tLibrary List and Opening File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot as plt\n",
    "import imblearn\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df2_with_dummies.csv\")\n",
    "df = df.drop(columns = ['Unnamed: 0','log_annual_inc', 'revol_bal', 'UR'])\n",
    "\n",
    "# defining target and feature variable\n",
    "X = df.drop(columns = 'charged_off')\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y = df['charged_off']\n",
    "\n",
    "# Separate train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size =.3,random_state=1234, stratify=y)\n",
    "print(\"The shape of train data set using df2_with_dummies is: x = {} and y = {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"The shape of test data set using df2_with_dummies is: x = {} and y = {}\". format(x_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define undersample strategy\n",
    "rus = RandomUnderSampler(sampling_strategy='majority')\n",
    "x_train_rus, y_train_rus = rus.fit_sample(x_train, y_train.ravel())\n",
    "\n",
    "#define smote strategy\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "x_train_smote, y_train_smote = smote.fit_sample(x_train, y_train.ravel())\n",
    "\n",
    "print(\"Before RUS, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before RUS, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "print(\"The shape of train data set using df2_with_dummies is: x = {} and y = {}\".format(x_train_res.shape, y_train_res.shape))\n",
    "print(\"The shape of test data set using df2_with_dummies is: x = {} and y = {} \\n\". format(x_test.shape, y_test.shape))\n",
    "\n",
    "print(\"After RUS, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After RUS, counts of label '0': {}\".format(sum(y_train_res==0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tNaive Bayesian Classifier Using df2_with_dummies Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model (object) for classification\n",
    "gnb_rus = GaussianNB()\n",
    "gnb_smote = GaussianNB()\n",
    "\n",
    "# Build a random forest classification model\n",
    "gnb_rus.fit(x_train_rus, y_train_rus)\n",
    "gnb_smote.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_gnb_rus = gnb_rus.predict(x_test)\n",
    "y_pred_gnb_smote = gnb_smote.predict(x_test)\n",
    "\n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm_nb_rus = metrics.confusion_matrix(y_test,y_pred_gnb_rus)\n",
    "print('\\nConfusion Matrix Naive Bayes - RUS','\\n',cm_nb_rus)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test,y_pred_gnb_rus))\n",
    "print(\"---------------------------------------------------------------------------------\") \n",
    "cm_nb_smote = metrics.confusion_matrix(y_test,y_pred_gnb_smote)\n",
    "print('\\nConfusion Matrix Naive Bayes - SMOTE','\\n',cm_nb_smote)\n",
    "print('\\nClassification Report Naive Bayes - SMOTE','\\n',metrics.classification_report(y_test,y_pred_gnb_smote))\n",
    "print(\"---------------------------------------------------------------------------------\") \n",
    "\n",
    "print(\"Performance score for NB - RUS \")\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_gnb_rus))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_gnb_rus))\n",
    "print ('Recall:', recall_score(y_test, y_pred_gnb_rus))\n",
    "print ('Precision:', precision_score(y_test, y_pred_gnb_rus))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_gnb_rus))\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Performance score for NB - SMOTE \")\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_gnb_smote))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_gnb_smote))\n",
    "print ('Recall:', recall_score(y_test, y_pred_gnb_smote))\n",
    "print ('Precision:', precision_score(y_test, y_pred_gnb_smote))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_gnb_smote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tDecision Trees Classifier Using df2_with_dummies Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of Decision Tree Classifier\n",
    "dtm_rus = DecisionTreeClassifier()\n",
    "dtm_smote = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the instance with training data\n",
    "dtm_rus.fit(x_train_rus, y_train_rus)\n",
    "dtm_smote.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "# Predict using the fitted model\n",
    "y_pred_dt_rus = dtm_rus.predict(x_test)\n",
    "y_pred_dt_smote = dtm_smote.predict(x_test)\n",
    "\n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm_dt_rus = metrics.confusion_matrix(y_test,y_pred_dt_rus)\n",
    "print('\\nConfusion Matrix DT RUS','\\n',cm_dt_rus)\n",
    "print('\\nClassification Report DECISION TREE RUS','\\n',metrics.classification_report(y_test,y_pred_dt_rus))\n",
    "print('--------------------------------------------------------------------------------')\n",
    "      \n",
    "cm_dt_smote = metrics.confusion_matrix(y_test,y_pred_dt_smote)\n",
    "print('\\nConfusion Matrix DT SMOTE','\\n',cm_dt_smote)\n",
    "print('\\nClassification Report DECISION TREE SMOTE','\\n',metrics.classification_report(y_test,y_pred_dt_smote))\n",
    "print('--------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tRandom Forest Classifier Using df2_with_dummies Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model (object) for classification\n",
    "rfcm_rus = RandomForestClassifier()\n",
    "rfcm_smote = RandomForestClassifier()\n",
    "\n",
    "# Build a random forest classification model\n",
    "rfcm_rus.fit(x_train_rus, y_train_rus)\n",
    "rfcm_smote.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_rf_rus = rfcm_rus.predict(x_test)\n",
    "y_pred_rf_smote = rfcm_smote.predict(x_test)\n",
    "\n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm_rf_rus = metrics.confusion_matrix(y_test,y_pred_rf_rus)\n",
    "print('\\nConfusion Matrix RF RUS','\\n',cm_rf_rus)\n",
    "print('\\nClassification Report RF RUS','\\n',metrics.classification_report(y_test,y_pred_rf_rus))\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "cm_rf_smote = metrics.confusion_matrix(y_test, y_pred_rf_smote)\n",
    "print('\\nConfusion Matrix RF SMOTE','\\n',cm_rf_smote)\n",
    "print('\\nClassification Report RF SMOTE','\\n',metrics.classification_report(y_test,y_pred_rf_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance score for RF - RUS \")\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_rf_rus))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_rf_rus))\n",
    "print ('Recall:', recall_score(y_test, y_pred_rf_rus))\n",
    "print ('Precision:', precision_score(y_test, y_pred_rf_rus))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_rf_rus))\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Performance score for RF - SMOTE \")\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_rf_smote))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_rf_smote))\n",
    "print ('Recall:', recall_score(y_test, y_pred_rf_smote))\n",
    "print ('Precision:', precision_score(y_test, y_pred_rf_smote))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_rf_smote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.\tNeural Network Classifier Using df2_with_dummies Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "Xn = scale(X)\n",
    "# Set the 'stratify' option 'y' to sample \n",
    "Xn_train, Xn_test = train_test_split(Xn, test_size =.3,random_state=1234, stratify=y)\n",
    "y_train, y_test = train_test_split(y, test_size=.3, random_state=1234, stratify=y)\n",
    "\n",
    "# define undersample strategy\n",
    "rus = RandomUnderSampler(sampling_strategy='majority')\n",
    "smote = SMOTE(sampling_strategy = 'minority')\n",
    "xn_train_rus, y_train_rus = rus.fit_sample(Xn_train, y_train.ravel())\n",
    "xn_train_smote, y_train_smote = smote.fit_sample(Xn_train, y_train.ravel())\n",
    "\n",
    "nnm_rus = MLPClassifier(hidden_layer_sizes=(20,), max_iter=1000,activation='logistic')\n",
    "nnm_smote = MLPClassifier(hidden_layer_sizes=(20,), max_iter=1000,activation='logistic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "nnm_rus.fit(xn_train_rus, y_train_rus)\n",
    "nnm_smote.fit(xn_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_nn_rus = nnm_rus.predict(Xn_test)\n",
    "y_pred_nn_smote = nnm_smote.predict(Xn_test)\n",
    "\n",
    "print('\\n ** Performance Scores **')\n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm_nn_rus = metrics.confusion_matrix(y_test,y_pred_nn_rus)\n",
    "print('\\nConfusion Matrix','\\n',cm_nn_rus)\n",
    "print('\\nClassification Report Neural Network - RUS','\\n',metrics.classification_report(y_test,y_pred_nn_rus))\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "cm_nn_smote = metrics.confusion_matrix(y_test,y_pred_nn_smote)\n",
    "print('\\nConfusion Matrix','\\n',cm_nn_smote)\n",
    "print('\\nClassification Report Neural Network - SMOTE','\\n',metrics.classification_report(y_test,y_pred_nn_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Accuracy:', accuracy_score(y_test, y_pred_nn_rus))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_nn_rus))\n",
    "print ('Recall:', recall_score(y_test, y_pred_nn_rus))\n",
    "print ('Precision:', precision_score(y_test, y_pred_nn_rus))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_nn_rus))\n",
    "print(\"--------------------------------------------\")\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_nn_smote))\n",
    "print ('F1 score:', f1_score(y_test, y_pred_nn_smote))\n",
    "print ('Recall:', recall_score(y_test, y_pred_nn_smote))\n",
    "print ('Precision:', precision_score(y_test, y_pred_nn_smote))\n",
    "print ('AUC:', roc_auc_score(y_test, y_pred_nn_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_nn_rus)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI.\tMachine Learning Algorithms Using K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the data frame after joining\n",
    "df2_with_dummies = pd.read_csv(\"df2_with_dummies.csv\", low_memory = False)\n",
    "# Removing that first column: (\"Unnamed: 0\")\n",
    "df2_with_dummies = df2_with_dummies.drop(columns = \"Unnamed: 0\")\n",
    "# Separate X from Y\n",
    "y = df2_with_dummies['charged_off']\n",
    "x = df2_with_dummies.drop(columns = ['charged_off'])\n",
    "x = x.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tNaive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of Gaussian Naive Bayesian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Gather all the scores from the cross-validation\n",
    "scores = cross_validate(gnb, x, y, cv = 5, scoring = ['accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'neg_root_mean_squared_error'])\n",
    "\n",
    "print(\"Average accuracy is: \", scores['test_accuracy'].mean(), \"with values as following: \", scores['test_accuracy'])\n",
    "print(\"Average F1 is: \", scores['test_f1'].mean(), \"with values as following: \", scores['test_f1'])\n",
    "print(\"Average recall/sensitivity is: \", scores['test_recall'].mean(), \"with values as following: \", scores['test_recall'])\n",
    "print(\"Average precision is: \", scores['test_precision'].mean(), \"with values as following: \", scores['test_precision'])\n",
    "print(\"Average ROC is: \", scores['test_roc_auc'].mean(), \"with values as following: \", scores['test_roc_auc'])\n",
    "print(\"Average RMSE is: \", -scores['test_neg_root_mean_squared_error'].mean(), \"with values as following: \", -scores['test_neg_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tDecision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate X from Y\n",
    "y = df2_with_dummies['charged_off']\n",
    "x = df2_with_dummies.drop(columns = ['charged_off'])\n",
    "x = x.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Create instance of Decision Tree Classifier\n",
    "dtm = DecisionTreeClassifier()\n",
    "\n",
    "# Get all the scores\n",
    "scores = cross_validate(dtm, x, y, cv = 5, scoring = ['accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'neg_root_mean_squared_error'])\n",
    "\n",
    "print(\"Average accuracy is: \", scores['test_accuracy'].mean())\n",
    "print(\"Average F1 is: \", scores['test_f1'].mean())\n",
    "print(\"Average recall/sensitivity is: \", scores['test_recall'].mean())\n",
    "print(\"Average precision is: \", scores['test_precision'].mean())\n",
    "print(\"Average ROC is: \", scores['test_roc_auc'].mean())\n",
    "print(\"Average RMSE is: \", -scores['test_neg_root_mean_squared_error'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.\tRandom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate X from Y\n",
    "y = df2_with_dummies['charged_off']\n",
    "x = df2_with_dummies.drop(columns = ['charged_off'])\n",
    "x = x.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "rfcm = RandomForestClassifier()\n",
    "\n",
    "scores = cross_validate(rfcm, x, y, cv = 5, scoring = ['accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'neg_root_mean_squared_error'])\n",
    "\n",
    "print(\"Average accuracy is: \", scores['test_accuracy'].mean(), \"with values as following: \", scores['test_accuracy'])\n",
    "print(\"Average F1 is: \", scores['test_f1'].mean(), \"with values as following: \", scores['test_f1'])\n",
    "print(\"Average recall/sensitivity is: \", scores['test_recall'].mean(), \"with values as following: \", scores['test_recall'])\n",
    "print(\"Average precision is: \", scores['test_precision'].mean(), \"with values as following: \", scores['test_precision'])\n",
    "print(\"Average ROC is: \", scores['test_roc_auc'].mean(), \"with values as following: \", scores['test_roc_auc'])\n",
    "print(\"Average RMSE is: \", -scores['test_neg_root_mean_squared_error'].mean(), \"with values as following: \", -scores['test_neg_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.\tNeural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2_with_dummies['charged_off']\n",
    "x = df2_with_dummies.drop(columns = ['charged_off'])\n",
    "x = x.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "nnm = MLPClassifier(hidden_layer_sizes=(20,), max_iter=2000,activation='logistic')\n",
    "\n",
    "scores = cross_validate(nnm, x, y, cv = 5, scoring = ['accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'neg_root_mean_squared_error'])\n",
    "\n",
    "print(\"Average accuracy is: \", scores['test_accuracy'].mean(), \"with values as following: \", scores['test_accuracy'])\n",
    "print(\"Average F1 is: \", scores['test_f1'].mean(), \"with values as following: \", scores['test_f1'])\n",
    "print(\"Average recall/sensitivity is: \", scores['test_recall'].mean(), \"with values as following: \", scores['test_recall'])\n",
    "print(\"Average precision is: \", scores['test_precision'].mean(), \"with values as following: \", scores['test_precision'])\n",
    "print(\"Average ROC is: \", scores['test_roc_auc'].mean(), \"with values as following: \", scores['test_roc_auc'])\n",
    "print(\"Average RMSE is: \", -scores['test_neg_root_mean_squared_error'].mean(), \"with values as following: \", -scores['test_neg_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII.\tArea Under Curve Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tLibrary List and Opening the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot as plt\n",
    "import imblearn\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import *\n",
    "import statsmodels.formula.api as smf\n",
    "import random\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df2_with_dummies.csv\")\n",
    "df = df.drop(columns = ['Unnamed: 0','log_annual_inc', 'revol_bal', 'UR'])\n",
    "\n",
    "# defining target and feature variable\n",
    "X = df.drop(columns = 'charged_off')\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y = df['charged_off']\n",
    "\n",
    "# Separate train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size =.3,random_state=1234, stratify=y)\n",
    "print(\"The shape of train data set using df2_with_dummies is: x = {} and y = {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"The shape of test data set using df2_with_dummies is: x = {} and y = {}\". format(x_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define undersample strategy\n",
    "rus = RandomUnderSampler(sampling_strategy='majority')\n",
    "x_train_rus, y_train_rus = rus.fit_sample(x_train, y_train.ravel())\n",
    "\n",
    "# Normalize the data\n",
    "Xn = scale(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.\tNeural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'stratify' option 'y' to sample \n",
    "Xn_train, Xn_test = train_test_split(Xn, test_size =.3,random_state=1234, stratify=y)\n",
    "y_train, y_test = train_test_split(y, test_size=.3, random_state=1234, stratify=y)\n",
    "\n",
    "# define undersample strategy\n",
    "rus = RandomUnderSampler(sampling_strategy='majority')\n",
    "# smote = SMOTE(sampling_strategy = 'minority')\n",
    "xn_train_rus, y_train_rus = rus.fit_sample(Xn_train, y_train.ravel())\n",
    "# xn_train_smote, y_train_smote = smote.fit_sample(Xn_train, y_train.ravel())\n",
    "\n",
    "nnm_rus = MLPClassifier(hidden_layer_sizes=(20,), max_iter=2000,activation='logistic')\n",
    "nnm_rus.fit(xn_train_rus, y_train_rus)\n",
    "\n",
    "y_pred_nn_rus = nnm_rus.predict(Xn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_pred_nn_rus)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.title('ROC Curve for NN - RUS', fontsize = 30)\n",
    "plt.plot(false_positive_rate1, true_positive_rate1)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate', fontsize = 20)\n",
    "plt.xlabel('False Positive Rate', fontsize = 20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'charged_off')\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y = df['charged_off']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =.3,random_state=1234, stratify=y)\n",
    "\n",
    " # Normalize the data\n",
    "Xn = scale(X)\n",
    " \n",
    "# Set the 'stratify' option 'y' to sample \n",
    "Xn_train, Xn_test = train_test_split(Xn, test_size =.3,random_state=1234, stratify=y)\n",
    "y_train, y_test = train_test_split(y, test_size=.3, random_state=1234, stratify=y)\n",
    " \n",
    "nnm = MLPClassifier(hidden_layer_sizes=(20,), max_iter=1000,activation='logistic')\n",
    "\n",
    "# Make predictions\n",
    "nnm.fit(Xn_train, y_train)\n",
    "y_pred_NN = nnm.predict(Xn_test)\n",
    "\n",
    "false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_pred_NN)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.title('ROC Curve for NN - Before RUS', fontsize = 30)\n",
    "plt.plot(false_positive_rate1, true_positive_rate1)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate', fontsize = 20)\n",
    "plt.xlabel('False Positive Rate', fontsize = 20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.\tRandom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model (object) for classification\n",
    "rfcm_pca = RandomForestClassifier()\n",
    "\n",
    "# Build a random forest classification model\n",
    "rfcm_pca.fit(X_train, y_train)\n",
    "y_pred_rf_pca = rfcm_pca.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_rf_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model (object) for classification\n",
    "rfcm_pca = RandomForestClassifier()\n",
    "\n",
    "# Build a random forest classification model\n",
    "rfcm_pca.fit(x_train_rus, y_train_rus)\n",
    "y_pred_rf_pca = rfcm_pca.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_rf_pca)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII.\tGetting Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)\n",
    "feat_importances = pd.Series(model.feature_importances_, index = X.columns)\n",
    "feat_importances.nlargest(10).plot(kind = 'barh')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII\tAttempting Clustering using K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create the k number of clusters after finding k using the elbow method. \n",
    "df_n = minmax_scale(X)\n",
    "df_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ssd = [] # Initialize the list for inertia values - sum of squared distances\n",
    "for i in range(2,50):\n",
    "    km = KMeans(n_clusters=i, random_state=1234)\n",
    "    km.fit(df_n)\n",
    "    ssd.append(km.inertia_)\n",
    " \n",
    "# Check the inertia values.\n",
    "for i in range(len(ssd)):\n",
    "    print('{0}: {1:.2f}'.format(i+2, ssd[i]))   \n",
    " \n",
    "# Draw the plot to find the elbow\n",
    "    \n",
    "plt.plot(range(2,50), ssd)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.show()\n",
    "\n",
    "km = KMeans(n_clusters=8, random_state=1234)\n",
    " \n",
    "# Create clusters\n",
    "km.fit(df_n)\n",
    "km.inertia_\n",
    " \n",
    "# Add the cluster number to the original data.\n",
    "df2_with_dummies['ClusterNo'] = km.labels_\n",
    "df2_with_dummies.head()\n",
    "\n",
    "# Divide the original data into the clusters.\n",
    " \n",
    "Cluster0 = df2_with_dummies.loc[df_with_dummies.ClusterNo == 0]\n",
    "Cluster0.describe()\n",
    "Cluster0.info()\n",
    " \n",
    "Cluster1 = df2_with_dummies.loc[df2_with_dummies.ClusterNo == 1]\n",
    "Cluster2 = df2_with_dummies.loc[df2_with_dummies.ClusterNo == 2]\n",
    "Cluster3 = df2_with_dummies.loc[df2_with_dummies.ClusterNo == 3]\n",
    "Cluster4 = df2_with_dummies.loc[df2_with_dummies.ClusterNo == 4]\n",
    "Cluster5 = df2_with_dummies.loc[df2_with_dummies.ClusterNo == 5]\n",
    "Cluster6 = df2_with_dummies.loc[df2_with_dummies.ClusterNo == 6]\n",
    "Cluster7 = df2_with_dummies.loc[df2_with_dummies.ClusterNo == 7]\n",
    "\n",
    "# Now, you can apply ml algorithms to each cluster.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of profiles of each cluster\n",
    "df2_with_dummies.groupby(['ClusterNo']).count()\n",
    "df2_with_dummies.groupby(['ClusterNo']).mean()\n",
    "df2_with_dummies.groupby(['ClusterNo']).median()\n",
    "df2_with_dummies.groupby(['ClusterNo']).max()\n",
    "df2_with_dummies.groupby(['ClusterNo']).min()\n",
    " \n",
    "# Set up X and y for each cluster\n",
    "X0 = Cluster0.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X0 = X.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y0 = Cluster0.charged_off\n",
    "X1 = Cluster1.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X1 = X1.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y1 = Cluster1.charged_off\n",
    "X2 = Cluster2.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X2 = X2.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y2 = Cluster2.charged_off\n",
    "X3 = Cluster3.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X3 = X3.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y3 = Cluster3.charged_off\n",
    "X4 = Cluster4.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X4 = X4.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y4 = Cluster4.charged_off\n",
    "X5 = Cluster5.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X5 = X5.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y5 = Cluster5.charged_off\n",
    "X6 = Cluster6.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X6 = X6.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y6 = Cluster6.charged_off\n",
    "X7 = Cluster7.drop(columns=['charged_off','ClusterNo'], axis = 1)\n",
    "X7 = X7.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "y7 = Cluster7.charged_off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split to train and test set\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X0,y0,test_size =.3,random_state=1234, stratify=y0)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1,y1,test_size =.3,random_state = 1234, stratify=y1)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2,test_size =.3,random_state=1234, stratify=y2)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3,y3,test_size =.3,random_state=1234, stratify=y3)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4,y4,test_size =.3,random_state=1234, stratify=y4)\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X5,y5,test_size =.3,random_state=1234, stratify=y5)\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X6,y6,test_size =.3,random_state=1234, stratify=y6)\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X7,y7,test_size =.3,random_state=1234, stratify=y7)\n",
    " \n",
    "# Create a model (object) for classification\n",
    "rfcm0 = RandomForestClassifier()\n",
    "rfcm1 = RandomForestClassifier()\n",
    "rfcm2 = RandomForestClassifier()\n",
    "rfcm3 = RandomForestClassifier()\n",
    "rfcm4 = RandomForestClassifier()\n",
    "rfcm5 = RandomForestClassifier()\n",
    "rfcm6 = RandomForestClassifier()\n",
    "rfcm7 = RandomForestClassifier()\n",
    " \n",
    "# Build random forest classification models\n",
    "rfcm0.fit(X_train0, y_train0)\n",
    "y_pred0 = rfcm0.predict(X_test0)\n",
    "rfcm1.fit(X_train1, y_train1)\n",
    "y_pred1 = rfcm1.predict(X_test1)\n",
    "rfcm2.fit(X_train2, y_train2)\n",
    "y_pred2 = rfcm2.predict(X_test2)\n",
    "rfcm3.fit(X_train3, y_train3)\n",
    "y_pred3 = rfcm3.predict(X_test3)\n",
    "rfcm4.fit(X_train4, y_train4)\n",
    "y_pred4 = rfcm4.predict(X_test4)\n",
    "rfcm5.fit(X_train5, y_train5)\n",
    "y_pred5 = rfcm5.predict(X_test5)\n",
    "rfcm6.fit(X_train6, y_train6)\n",
    "y_pred6 = rfcm6.predict(X_test6)\n",
    "rfcm7.fit(X_train7, y_train7)\n",
    "y_pred7 = rfcm7.predict(X_test7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n",
    "\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy0 = rfcm0.score(X_test0, y_test0)\n",
    "print('Accuracy0: {0:.2f}'.format(accuracy0))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm0 = metrics.confusion_matrix(y_test0,y_pred0)\n",
    "print('\\nConfusion Matrix','\\n',cm0)\n",
    "\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test0,y_pred0))\n",
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy1 = rfcm1.score(X_test1, y_test1)\n",
    "print('Accuracy1: {0:.2f}'.format(accuracy1))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm1 = metrics.confusion_matrix(y_test1,y_pred1)\n",
    "print('\\nConfusion Matrix','\\n',cm1)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test1,y_pred1))\n",
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy2 = rfcm2.score(X_test2, y_test2)\n",
    "print('Accuracy2: {0:.2f}'.format(accuracy2))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm2 = metrics.confusion_matrix(y_test2,y_pred2)\n",
    "print('\\nConfusion Matrix','\\n',cm2)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test2,y_pred2))\n",
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy3 = rfcm3.score(X_test3, y_test3)\n",
    "print('Accuracy3: {0:.2f}'.format(accuracy3))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm3 = metrics.confusion_matrix(y_test3,y_pred3)\n",
    "print('\\nConfusion Matrix','\\n',cm3)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test3,y_pred3))\n",
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy4 = rfcm4.score(X_test4, y_test4)\n",
    "print('Accuracy4: {0:.2f}'.format(accuracy4))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm4 = metrics.confusion_matrix(y_test4,y_pred4)\n",
    "print('\\nConfusion Matrix','\\n',cm4)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test4,y_pred4))\n",
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy5 = rfcm5.score(X_test5, y_test5)\n",
    "print('Accuracy5: {0:.2f}'.format(accuracy5))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm5 = metrics.confusion_matrix(y_test5,y_pred5)\n",
    "print('\\nConfusion Matrix','\\n',cm)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test5,y_pred5))\n",
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy6 = rfcm6.score(X_test6, y_test6)\n",
    "print('Accuracy6: {0:.2f}'.format(accuracy6))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm6 = metrics.confusion_matrix(y_test6,y_pred6)\n",
    "print('\\nConfusion Matrix','\\n',cm6)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test6,y_pred6))\n",
    " \n",
    "# Print the performance \n",
    "print('\\n ** Performance Scores **')\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy7 = rfcm7.score(X_test7, y_test7)\n",
    "print('Accuracy7: {0:.2f}'.format(accuracy7))\n",
    " \n",
    "# Build a confusion matrix and show the Classification Report\n",
    "cm7 = metrics.confusion_matrix(y_test7,y_pred7)\n",
    "print('\\nConfusion Matrix','\\n',cm7)\n",
    "print('\\nClassification Report','\\n',metrics.classification_report(y_test7,y_pred7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bayesian Classifier instance for classification\n",
    "gnb0 = GaussianNB()\n",
    "gnb1 = GaussianNB()\n",
    "gnb2 = GaussianNB()\n",
    "gnb3 = GaussianNB()\n",
    "gnb4 = GaussianNB()\n",
    "gnb5 = GaussianNB()\n",
    "gnb6 = GaussianNB()\n",
    "gnb7 = GaussianNB()\n",
    " \n",
    "gnb0.fit(X_train0, y_train0)\n",
    "y_pred0 = gnb0.predict(X_test0)\n",
    "gnb1.fit(X_train1, y_train1)\n",
    "y_pred1 = gnb1.predict(X_test1)\n",
    "gnb2.fit(X_train2, y_train2)\n",
    "y_pred2 = gnb2.predict(X_test2)\n",
    "gnb3.fit(X_train3, y_train3)\n",
    "y_pred3 = gnb3.predict(X_test3)\n",
    "gnb4.fit(X_train4, y_train4)\n",
    "y_pred4 = gnb4.predict(X_test4)\n",
    "gnb5.fit(X_train5, y_train5)\n",
    "y_pred5 = gnb5.predict(X_test5)\n",
    "gnb6.fit(X_train6, y_train6)\n",
    "y_pred6 = gnb6.predict(X_test6)\n",
    "gnb7.fit(X_train7, y_train7)\n",
    "y_pred7 = gnb7.predict(X_test7)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy0 = gnb0.score(X_test0, y_test0)\n",
    "print('Accuracy0: {0:.2f}'.format(accuracy0))\n",
    "accuracy1 = gnb1.score(X_test1, y_test1)\n",
    "print('Accuracy1: {0:.2f}'.format(accuracy1))\n",
    "accuracy2 = gnb2.score(X_test2, y_test2)\n",
    "print('Accuracy2: {0:.2f}'.format(accuracy2))\n",
    "accuracy3 = gnb3.score(X_test3, y_test3)\n",
    "print('Accuracy3: {0:.2f}'.format(accuracy3))\n",
    "accuracy4 = gnb4.score(X_test4, y_test4)\n",
    "print('Accuracy4: {0:.2f}'.format(accuracy4))\n",
    "accuracy5 = gnb5.score(X_test5, y_test5)\n",
    "print('Accuracy5: {0:.2f}'.format(accuracy5))\n",
    "accuracy6 = gnb6.score(X_test6, y_test6)\n",
    "print('Accuracy6: {0:.2f}'.format(accuracy6))\n",
    "accuracy7 = gnb7.score(X_test7, y_test7)\n",
    "print('Accuracy7: {0:.2f}'.format(accuracy7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a confusion matrix\n",
    "cm0 = metrics.confusion_matrix(y_test0,y_pred0)\n",
    "print(metrics.classification_report(y_test0,y_pred0))\n",
    "cm1 = metrics.confusion_matrix(y_test1,y_pred1)\n",
    "print(metrics.classification_report(y_test1,y_pred1))\n",
    "cm2 = metrics.confusion_matrix(y_test2,y_pred2)\n",
    "print(metrics.classification_report(y_test2,y_pred2))\n",
    "cm3 = metrics.confusion_matrix(y_test3,y_pred3)\n",
    "print(metrics.classification_report(y_test3,y_pred3))\n",
    "cm4 = metrics.confusion_matrix(y_test4,y_pred4)\n",
    "print(metrics.classification_report(y_test4,y_pred4))\n",
    "cm5 = metrics.confusion_matrix(y_test5,y_pred5)\n",
    "print(metrics.classification_report(y_test5,y_pred5))\n",
    "cm6 = metrics.confusion_matrix(y_test6,y_pred6)\n",
    "print(metrics.classification_report(y_test6,y_pred6))\n",
    "cm7 = metrics.confusion_matrix(y_test7,y_pred7)\n",
    "print(metrics.classification_report(y_test7,y_pred7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
